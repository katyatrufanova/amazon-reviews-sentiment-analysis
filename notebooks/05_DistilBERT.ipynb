{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A5tXQcapv_EM",
        "outputId": "5cabe263-4afc-4d8a-8a27-21171e8540f3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.30.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.16.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import DistilBertTokenizer, TFDistilBertModel, DistilBertConfig\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import classification_report"
      ],
      "metadata": {
        "id": "TvMSBN4gvYZC"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6KAigTgZvYP3",
        "outputId": "8bc90be5-e70f-4c4c-ce39-2acf840eb85d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = '/content/drive/MyDrive/ML/Amazon_Unlocked_Mobile.csv'\n",
        "data = pd.read_csv(file_path)"
      ],
      "metadata": {
        "id": "mfsXfSHyvYJw"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = data.dropna()"
      ],
      "metadata": {
        "id": "WURzFFZWvX5W"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    text = re.sub(r'<.*?>', '', text)\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    text = \" \".join([word for word in text.split() if word not in stopwords.words('english')])\n",
        "    return text"
      ],
      "metadata": {
        "id": "tJACOH9VveNq"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z4ZQNoJeveGi",
        "outputId": "c228675a-c43f-4e0d-a623-384fc76ae01f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data['CleanedReviews'] = data['Reviews'].apply(clean_text)"
      ],
      "metadata": {
        "id": "rmpvdZxJvd_Q"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Label encoding\n",
        "def sentiment(rating):\n",
        "    if rating < 3:\n",
        "        return 'negative'\n",
        "    elif rating == 3:\n",
        "        return 'neutral'\n",
        "    else:\n",
        "        return 'positive'\n",
        "\n",
        "data['Sentiment'] = data['Rating'].apply(sentiment)"
      ],
      "metadata": {
        "id": "5KLFEzXWvXwX"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split dataset into training and testing sets\n",
        "X = data['CleanedReviews']\n",
        "y = data['Sentiment']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "kgxO-PSUviEp"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization and padding\n",
        "max_length = 128\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "X_train_encoded = tokenizer(list(X_train), padding=True, truncation=True, max_length=max_length, return_tensors='tf').data\n",
        "X_test_encoded = tokenizer(list(X_test), padding=True, truncation=True, max_length=max_length, return_tensors='tf').data"
      ],
      "metadata": {
        "id": "pk-opPnxvio6"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom model with DistilBert layer\n",
        "class SentimentModel(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super(SentimentModel, self).__init__()\n",
        "        self.distilbert = TFDistilBertModel.from_pretrained('distilbert-base-uncased')\n",
        "        self.dense = tf.keras.layers.Dense(3, activation='softmax')\n",
        "\n",
        "    def call(self, inputs):\n",
        "        output = self.distilbert(inputs)\n",
        "        pooled_output = output[0][:, 0, :]\n",
        "        return self.dense(pooled_output)"
      ],
      "metadata": {
        "id": "7qILXf4rviV4"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = SentimentModel()\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
        "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_1aqp-Scvwrx",
        "outputId": "01271c31-c6e2-4070-8cff-c478f3aec65c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertModel: ['vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_transform.weight']\n",
            "- This IS expected if you are initializing TFDistilBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of TFDistilBertModel were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "y_train_encoded = label_encoder.fit_transform(y_train)\n",
        "y_test_encoded = label_encoder.transform(y_test)"
      ],
      "metadata": {
        "id": "0Kswn0E0vwlQ"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
        "history = model.fit(X_train_encoded, y_train_encoded, epochs=5, batch_size=16, validation_split=0.2, callbacks=[early_stop])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "znexzxXEvwcw",
        "outputId": "474e6330-f5c2-4653-845d-6a57c0298d35"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "13374/13374 [==============================] - 3102s 229ms/step - loss: 0.3481 - accuracy: 0.8766 - val_loss: 0.2871 - val_accuracy: 0.9013\n",
            "Epoch 2/5\n",
            "13374/13374 [==============================] - 2985s 223ms/step - loss: 0.2408 - accuracy: 0.9184 - val_loss: 0.2544 - val_accuracy: 0.9158\n",
            "Epoch 3/5\n",
            "13374/13374 [==============================] - 2983s 223ms/step - loss: 0.1782 - accuracy: 0.9424 - val_loss: 0.2472 - val_accuracy: 0.9245\n",
            "Epoch 4/5\n",
            "13374/13374 [==============================] - 2982s 223ms/step - loss: 0.1384 - accuracy: 0.9566 - val_loss: 0.2373 - val_accuracy: 0.9287\n",
            "Epoch 5/5\n",
            "13374/13374 [==============================] - 2964s 222ms/step - loss: 0.1144 - accuracy: 0.9648 - val_loss: 0.2783 - val_accuracy: 0.9291\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5mhPb54xu47-",
        "outputId": "35eda9b3-f525-45ed-e36c-14ed81f2e752"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2090/2090 [==============================] - 289s 137ms/step\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.91      0.90      0.91     15609\n",
            "     neutral       0.70      0.65      0.67      5228\n",
            "    positive       0.96      0.97      0.97     46030\n",
            "\n",
            "    accuracy                           0.93     66867\n",
            "   macro avg       0.86      0.84      0.85     66867\n",
            "weighted avg       0.93      0.93      0.93     66867\n",
            "\n"
          ]
        }
      ],
      "source": [
        "y_pred = model.predict(X_test_encoded)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "\n",
        "print(\"Classification Report:\\n\", classification_report(y_test_encoded, y_pred_classes, target_names=label_encoder.classes_))"
      ]
    }
  ]
}